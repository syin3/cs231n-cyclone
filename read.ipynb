{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "Run 1st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.1.post2\n",
      "Torchvision Version:  0.2.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import skimage\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Deep Learning model\n",
    "Run 2nd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "#            'resnet152']\n",
    "\n",
    "\n",
    "# model_urls = {\n",
    "#     'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "#     'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "#     'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "#     'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "#     'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "# }\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        ###\n",
    "        ### change 3 to 2 for our data\n",
    "        ###\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### classic AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class classicAlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(classicAlexNet, self).__init__()\n",
    "        ###\n",
    "        ### Changed 3 to 4 to fit our data dimension sieze\n",
    "        ###\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    model = classicAlexNet(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['alexnet'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modified AlexNet by Paper (kddTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notice data fed to kddTC should follow the pre-processing steps by the paper\n",
    "Especially, (1) only two channels should be included;\n",
    "(2) middle cropping;\n",
    "(3) rotation?\n",
    "'''\n",
    "class kddAlex(nn.Module):\n",
    "\n",
    "    def __init__(self, num_outputs = 1):\n",
    "        super(kddAlex, self).__init__()\n",
    "        ###\n",
    "        ### Changed 3 to 2 to fit our data dimension sieze\n",
    "        ###\n",
    "        self.features = nn.Sequential(\n",
    "            # nn.Conv2d(2, 64, kernel_size=11, stride=4, padding=2),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(2, 16, kernel_size=4, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(128 * 3 * 3, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 128 * 3 * 3)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def kddTC(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    model = kddAlex(**kwargs)\n",
    "    # if pretrained:\n",
    "    #    state_dict = load_state_dict_from_url(model_urls['alexnet'],\n",
    "    #                                          progress=progress)\n",
    "    #    model.load_state_dict(state_dict)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Before the model, obtain enough info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load in data from h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_path = \"TCIR-ALL_2017.h5\"\n",
    "\n",
    "# load \"info\" as pandas dataframe\n",
    "data_info = pd.read_hdf(data_path, key=\"info\", mode='r')\n",
    "\n",
    "# load \"matrix\" as numpy ndarray, this could take longer times\n",
    "with h5py.File(data_path, 'r') as hf:\n",
    "    data_matrix = hf['matrix'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_matrix.reshape(-1, 4).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "skimage.transform.rotate(data_matrix[0, :, :, :], 10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.hist(data_matrix[0, :, :, 0].flatten(), 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.hist(skimage.transform.rotate(data_matrix[0, :, :, 0], 1, preserve_range=True).flatten(), 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "plt.hist(scipy.ndimage.rotate(data_matrix[0, :, :, 0], 1).flatten(), 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(data_matrix[0, :, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(skimage.transform.rotate(data_matrix[0, :, :, 0], 90, preserve_range=True), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.max(data_matrix[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.max(skimage.transform.rotate(data_matrix[0, :, :, 0], 10, preserve_range=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Check how many in matrix is nan\n",
    "Must deal with data outlier as proposed in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.prod(data_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(data_matrix.shape[0]):\n",
    "    if np.count_nonzero(np.isnan(data_matrix[i, :, :, :])) > 0:\n",
    "        count += 1\n",
    "print(count)\n",
    "print(data_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### A sketch of label matrix\n",
    "Vmax is wind speed, MSLP is minimum sea level pressure, R35_4qAVG is radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Several visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(data_info.loc[data_info['ID'] == '201733W']['Vmax'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(data_matrix[0,:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(data_matrix[1,:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(data_matrix[2,:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(data_matrix[10,:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Obtain mean and variance of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a model with Pytorch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Check h5df file\n",
    "Don't run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def h5printR(item, leading = ''):\n",
    "    for key in item:\n",
    "        if isinstance(item[key], h5py.Dataset):\n",
    "            print(leading + key + ': ' + str(item[key].shape))\n",
    "        else:\n",
    "            print(leading + key)\n",
    "            h5printR(item[key], leading + '  ')\n",
    "\n",
    "# Print structure of a `.h5` file            \n",
    "def h5print(filename):\n",
    "    with h5py.File(filename, 'r') as h:\n",
    "        print(filename)\n",
    "        h5printR(h, '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_path = 'TCIR-ALL_2017.h5'\n",
    "h5_file = h5py.File(file_path)\n",
    "matrix = h5_file.get('matrix')\n",
    "target = h5_file.get('info') #/block0_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list(target['block0_items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h5print('TCIR-ALL_2017.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "datMatrix = torch.from_numpy(matrix[:, 68 : 68 + 64, 68 : 68 + 64, [0,3]]).permute(0, 3, 1, 2).float()\n",
    "datMatrix = np.nan_to_num(datMatrix)\n",
    "datMatrix[datMatrix > 1000] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(np.isnan(datMatrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue: data load, model train\n",
    "+ **Run 3rd**.<br>\n",
    "+ Dimension does not match. Pytorch defaults to $N\\times C\\times H\\times W$, while our data comes in $N\\times H\\times W\\times C\\$. Use **permute()** to swap axis. <br>\n",
    "+ Next, we have four channels while resnet only accepts 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kddTCData(data.Dataset):\n",
    "\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        super(kddTCData, self).__init__()\n",
    "        h5_file = h5py.File(file_path)\n",
    "        self.data = h5_file.get('matrix')\n",
    "        # hard code the Vmax label\n",
    "        self.target = h5_file.get('info/block0_values')[:,2]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # middle crop and permute axis\n",
    "        datMatrix = torch.from_numpy(self.data[index, 68 : (68 + 64), 68 : (68 + 64), [0,3]]).permute(2, 0, 1).float()\n",
    "        # [0,3]\n",
    "        # datMatrix = (datMatrix - datMatrix.mean(axis=0)) / datMatrix.std(axis=0)\n",
    "        labMatrix = torch.from_numpy(self.target)[index].float()\n",
    "        # replace nan with 0\n",
    "        datMatrix = np.nan_to_num(datMatrix)\n",
    "        # replace extremely large values with 0\n",
    "        datMatrix[datMatrix > 1000] = 0\n",
    "        return (datMatrix, labMatrix)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def length(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, length, is_inception=False):\n",
    "    since = time.time()\n",
    "    \n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    # best_acc = 0.0\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            # running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        # print(outputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / length[phase]\n",
    "            # epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            # print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            if phase == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_loss_history, train_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, data_source, batch_size, num_epochs):\n",
    "    dataDic = {\n",
    "        '2017': 'TCIR-ALL_2017.h5',\n",
    "        'c-i-sh': 'TCIR-CPAC_IO_SH.h5',\n",
    "        'a-e-w':'TCIR-ATLN_EPAC_WPAC.h5'\n",
    "    }\n",
    "    transf = {\n",
    "        '2017': {'mean': None, 'std': None},\n",
    "        'c-i-sh': {'mean': None, 'std': None}, \n",
    "        'a-e-w': {'mean': None, 'std': None}\n",
    "    }\n",
    "    data_transform = transforms.Normalize(mean=transf[data_source]['mean'], std=transf[data_source]['std'])\n",
    "    data = kddTCData(dataDic[data_source],\n",
    "                         transform = data_transform)\n",
    "    \n",
    "    NUM_DATA = 500 # data.length()\n",
    "    print(NUM_DATA)\n",
    "    length = {}\n",
    "    length['train'] = int(NUM_DATA * 0.7)\n",
    "    length['val'] = NUM_DATA - int(NUM_DATA * 0.7)\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = DataLoader(data, batch_size=batch_size, \n",
    "                                      sampler=SubsetRandomSampler(range(int(NUM_DATA * 0.7))),\n",
    "                             num_workers=4)\n",
    "    dataloaders['val'] = DataLoader(data, batch_size=batch_size, \n",
    "                                    sampler=SubsetRandomSampler(range(int(NUM_DATA * 0.7), NUM_DATA)),\n",
    "                            num_workers=4)\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    # optim.SGD(kdd18.parameters(), lr=0.001, momentum=0.9)\n",
    "    model_optmzer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)\n",
    "\n",
    "    model, val_loss_history, train_loss_history = train_model(model, dataloaders, criterion, \n",
    "                           model_optmzer, num_epochs=num_epochs, length = length)\n",
    "    return model, val_loss_history, train_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 5227.9162\n",
      "val Loss: 657.3753\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 5091.9267\n",
      "val Loss: 595.9129\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 4952.6401\n",
      "val Loss: 517.6637\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 4750.6253\n",
      "val Loss: 411.2284\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 4479.4202\n",
      "val Loss: 277.9759\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 4098.7953\n",
      "val Loss: 132.6577\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 3605.5051\n",
      "val Loss: 31.7875\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 2997.6089\n",
      "val Loss: 128.9681\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 2398.8438\n",
      "val Loss: 733.8793\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 2116.7122\n",
      "val Loss: 1990.0893\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 2260.5772\n",
      "val Loss: 2493.8110\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 2301.9771\n",
      "val Loss: 1909.2713\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 2154.6664\n",
      "val Loss: 1173.9405\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 2087.1091\n",
      "val Loss: 774.9469\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 2101.6597\n",
      "val Loss: 653.3036\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 2134.5640\n",
      "val Loss: 656.4777\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 2114.4096\n",
      "val Loss: 816.0032\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 2085.6659\n",
      "val Loss: 1055.8312\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 2071.5208\n",
      "val Loss: 1272.0729\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 2085.3277\n",
      "val Loss: 1353.5197\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 2085.9876\n",
      "val Loss: 1265.0861\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 2086.6634\n",
      "val Loss: 1092.6769\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 2078.5929\n",
      "val Loss: 995.4614\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 2084.5437\n",
      "val Loss: 930.0061\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 2080.9291\n",
      "val Loss: 985.2992\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 2073.0009\n",
      "val Loss: 1054.3865\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 2070.8987\n",
      "val Loss: 1114.3988\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 2075.0825\n",
      "val Loss: 1170.4516\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 2074.5773\n",
      "val Loss: 1175.9559\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 2075.9845\n",
      "val Loss: 1120.0711\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 2073.0798\n",
      "val Loss: 1040.2885\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 2073.6987\n",
      "val Loss: 1008.6776\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 2075.6318\n",
      "val Loss: 1005.7450\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 2076.2753\n",
      "val Loss: 1020.3267\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 2075.6758\n",
      "val Loss: 1088.6559\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 2074.8410\n",
      "val Loss: 1077.9123\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 2076.2706\n",
      "val Loss: 1151.7855\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 2072.9757\n",
      "val Loss: 1130.9067\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 2074.3937\n",
      "val Loss: 1107.2109\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 2072.4305\n",
      "val Loss: 1049.8726\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 2072.9419\n",
      "val Loss: 1022.2183\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 2075.4426\n",
      "val Loss: 1043.5033\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 2077.2913\n",
      "val Loss: 1077.3359\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 2073.6574\n",
      "val Loss: 1085.1251\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 2079.0506\n",
      "val Loss: 1048.0777\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 2079.8209\n",
      "val Loss: 1068.0278\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 2086.2985\n",
      "val Loss: 1092.2004\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 2071.3629\n",
      "val Loss: 1080.8835\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 2072.5787\n",
      "val Loss: 1108.0452\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 2077.8235\n",
      "val Loss: 1098.4313\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 2075.8216\n",
      "val Loss: 1153.1407\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 2073.1242\n",
      "val Loss: 1112.3824\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 2075.8965\n",
      "val Loss: 1033.2268\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 2072.4738\n",
      "val Loss: 1065.1639\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 2094.4149\n",
      "val Loss: 1003.6770\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 2082.7832\n",
      "val Loss: 1148.3096\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 2091.3345\n",
      "val Loss: 1187.2576\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 2070.3067\n",
      "val Loss: 1062.2225\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 2083.3080\n",
      "val Loss: 919.5818\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 2078.6492\n",
      "val Loss: 958.5248\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 2071.9771\n",
      "val Loss: 1079.0223\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 2070.4901\n",
      "val Loss: 1177.9190\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 2073.1033\n",
      "val Loss: 1209.4271\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 2081.0081\n",
      "val Loss: 1134.3653\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 2071.3287\n",
      "val Loss: 1093.8046\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 2074.8346\n",
      "val Loss: 1031.7228\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 2070.7505\n",
      "val Loss: 1053.2912\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 2072.0479\n",
      "val Loss: 1068.7810\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 2074.6008\n",
      "val Loss: 1049.1992\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 2069.5008\n",
      "val Loss: 1064.8717\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 2072.7358\n",
      "val Loss: 1123.0537\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 2069.4791\n",
      "val Loss: 1097.2910\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 2070.4434\n",
      "val Loss: 1080.0695\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 2073.8796\n",
      "val Loss: 1004.8891\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 2071.2847\n",
      "val Loss: 1017.4622\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 2066.6134\n",
      "val Loss: 1030.6286\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 2076.9227\n",
      "val Loss: 1080.3763\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 2066.8309\n",
      "val Loss: 1049.7391\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 2066.5371\n",
      "val Loss: 1002.0439\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 2067.0444\n",
      "val Loss: 978.3636\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 2066.5202\n",
      "val Loss: 1035.8299\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 2070.3309\n",
      "val Loss: 1076.1289\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 2064.7193\n",
      "val Loss: 1177.7950\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 2067.8000\n",
      "val Loss: 1132.2392\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 2065.2058\n",
      "val Loss: 1064.1697\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 2066.0556\n",
      "val Loss: 1000.3680\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 2071.0264\n",
      "val Loss: 1049.7689\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 2062.0512\n",
      "val Loss: 1004.0206\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 2071.7735\n",
      "val Loss: 950.3825\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 2067.9857\n",
      "val Loss: 1028.7832\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 2063.6582\n",
      "val Loss: 1187.2876\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 2073.0398\n",
      "val Loss: 1231.1850\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 2065.6663\n",
      "val Loss: 1064.4500\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 2067.3757\n",
      "val Loss: 889.7655\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 2066.8897\n",
      "val Loss: 933.5331\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 2067.5283\n",
      "val Loss: 1057.7182\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 2062.3759\n",
      "val Loss: 1103.2539\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 2067.1825\n",
      "val Loss: 1037.8282\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 2066.6489\n",
      "val Loss: 1058.2614\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 2066.5266\n",
      "val Loss: 988.0896\n",
      "\n",
      "Training complete in 2m 0s\n",
      "Best val Loss: 31.787533\n"
     ]
    }
   ],
   "source": [
    "kdd18 = kddTC(pretrained=False)\n",
    "_, val_loss, train_loss = learn(kdd18, 'c-i-sh', batch_size = 128, num_epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(val_loss, label = 'val')\n",
    "plt.plot(train_loss, label = 'train')\n",
    "plt.title('Losses curve on (500 *0.7) training dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(val_loss, label = 'val')\n",
    "plt.plot(train_loss, label = 'train')\n",
    "plt.title('Losses curve on (150 *0.7) training dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "resnet = resnet34(pretrained=False)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "num_output = 1\n",
    "resnet.fc = nn.Linear(num_ftrs, num_output)\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "resnet_optmzer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "resnet = train_model(resnet, dataloaders, criterion, \n",
    "                       resnet_optmzer, num_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
